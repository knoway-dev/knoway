package openai

import (
	"bufio"
	"bytes"
	"encoding/json"
	"errors"
	"fmt"
	"log/slog"
	"net/http"

	"knoway.dev/pkg/object"
	"knoway.dev/pkg/utils"
)

type CompletionTokensDetails struct {
	AcceptedPredictionTokens uint64 `json:"accepted_prediction_tokens"` // When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion.
	AudioTokens              uint64 `json:"audio_tokens"`               // Audio input tokens generated by the model.
	ReasoningTokens          uint64 `json:"reasoning_tokens"`           // Tokens generated by the model for reasoning.
	RejectedPredictionTokens uint64 `json:"rejected_prediction_tokens"` // When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits.
}

type PromptTokensDetails struct {
	AudioTokens  uint64 `json:"audio_tokens"`  // Audio input tokens generated by the model.
	CachedTokens uint64 `json:"cached_tokens"` // Tokens generated by the model that were cached from previous completions.
}

var _ object.LLMUsage = (*Usage)(nil)

type Usage struct {
	TotalTokens             uint64                   `json:"total_tokens,omitempty"`              // Total number of tokens used in the request (prompt + completion).
	CompletionTokens        uint64                   `json:"completion_tokens,omitempty"`         // Number of tokens in the generated completion.
	PromptTokens            uint64                   `json:"prompt_tokens,omitempty"`             // Number of tokens in the prompt.
	CompletionTokensDetails *CompletionTokensDetails `json:"completion_tokens_details,omitempty"` // Breakdown of tokens used in a completion.
	PromptTokensDetails     *PromptTokensDetails     `json:"prompt_tokens_details,omitempty"`     // Breakdown of tokens used in the prompt.
}

func (u *Usage) GetTotalTokens() uint64 {
	return u.TotalTokens
}

func (u *Usage) GetCompletionTokens() uint64 {
	return u.CompletionTokens
}

func (u *Usage) GetPromptTokens() uint64 {
	return u.PromptTokens
}

var _ object.LLMResponse = (*ChatCompletionsResponse)(nil)

type ChatCompletionsResponse struct {
	Status int            `json:"status"`
	Model  string         `json:"model"`
	Usage  *Usage         `json:"usage,omitempty"`
	Error  *ErrorResponse `json:"error,omitempty"`
	Stream bool           `json:"stream"`

	request          object.LLMRequest
	responseBody     json.RawMessage
	bodyParsed       map[string]any
	outgoingResponse *http.Response
}

func NewChatCompletionResponse(request object.LLMRequest, response *http.Response, reader *bufio.Reader) (*ChatCompletionsResponse, error) {
	resp := new(ChatCompletionsResponse)

	buffer := new(bytes.Buffer)

	_, err := buffer.ReadFrom(reader)
	if err != nil {
		return nil, err
	}

	err = resp.processBytes(buffer.Bytes(), response)
	if err != nil {
		return nil, fmt.Errorf("failed to unmarshal response: %w", err)
	}

	resp.request = request
	resp.outgoingResponse = response

	return resp, nil
}

func (r *ChatCompletionsResponse) processBytes(bs []byte, response *http.Response) error {
	if r == nil {
		return nil
	}

	r.responseBody = bs
	r.Status = response.StatusCode

	var body map[string]any

	err := json.Unmarshal(bs, &body)
	if err != nil {
		return fmt.Errorf("failed to unmarshal response body: %w", err)
	}

	r.bodyParsed = body

	r.Model = utils.GetByJSONPath[string](body, "{ .model }")
	usageMap := utils.GetByJSONPath[map[string]any](body, "{ .usage }")

	// For general cases, errors will be returned as a map with "error" property
	respErrMap := utils.GetByJSONPath[map[string]any](body, "{ .error }")
	// For OpenRouter, endpoint not found errors will be returned as a string with "error" property
	errorStringMap := utils.GetByJSONPath[string](body, "{ .error }")

	r.Usage, err = utils.FromMap[Usage](usageMap)
	if err != nil {
		return fmt.Errorf("failed to unmarshal usage: %w", err)
	}

	if len(respErrMap) > 0 {
		respErr, err := utils.FromMap[ErrorResponse](respErrMap)
		if err != nil {
			return fmt.Errorf("failed to unmarshal error: %w", err)
		}

		respErr.Status = response.StatusCode
		respErr.FromUpstream = true
		r.Error = respErr
	} else if errorStringMap != "" {
		slog.Error("unknown unexpected error response returned",
			slog.String("body", string(bs)),
			slog.String("uri", response.Request.RequestURI),
			slog.String("url", response.Request.URL.String()),
		)

		respErr := &ErrorResponse{
			Status: response.StatusCode,
			ErrorBody: &Error{
				Message: "upstream error: " + errorStringMap,
			},
			Cause: errors.New("unknown error"),
		}

		respErr.FromUpstream = true
		r.Error = respErr
	} else if response.StatusCode >= 400 && response.StatusCode < 600 {
		slog.Error("unknown unexpected error response with unknown body structure returned",
			slog.String("body", string(bs)),
			slog.String("uri", response.Request.RequestURI),
			slog.String("url", response.Request.URL.String()),
		)

		respErr := &ErrorResponse{
			Status: response.StatusCode,
			ErrorBody: &Error{
				Message: "upstream unknown error: " + response.Status,
			},
			Cause: errors.New("unknown error"),
		}

		respErr.FromUpstream = true
		r.Error = respErr
	}

	return nil
}

func (r *ChatCompletionsResponse) MarshalJSON() ([]byte, error) {
	return r.responseBody, nil
}

func (r *ChatCompletionsResponse) IsStream() bool {
	return false
}

func (r *ChatCompletionsResponse) GetRequestID() string {
	// TODO: implement
	return ""
}

func (r *ChatCompletionsResponse) GetModel() string {
	return r.Model
}

func (r *ChatCompletionsResponse) SetModel(model string) error {
	if r.Error == nil {
		var err error

		r.responseBody, r.bodyParsed, err = modifyBytesBodyAndParsed(r.responseBody, NewReplace("/model", model))
		if err != nil {
			return err
		}
	}

	r.Model = model

	return nil
}

func (r *ChatCompletionsResponse) GetUsage() object.LLMUsage {
	return r.Usage
}

func (r *ChatCompletionsResponse) GetError() object.LLMError {
	if r.Error != nil {
		return r.Error
	}

	return nil
}
