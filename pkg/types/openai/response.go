package openai

import (
	"bufio"
	"bytes"
	"encoding/json"
	"fmt"
	"net/http"

	"knoway.dev/pkg/object"
	"knoway.dev/pkg/utils"
)

type CompletionTokensDetails struct {
	AcceptedPredictionTokens uint64 `json:"accepted_prediction_tokens"` // When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion.
	AudioTokens              uint64 `json:"audio_tokens"`               // Audio input tokens generated by the model.
	ReasoningTokens          uint64 `json:"reasoning_tokens"`           // Tokens generated by the model for reasoning.
	RejectedPredictionTokens uint64 `json:"rejected_prediction_tokens"` // When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits.
}

type PromptTokensDetails struct {
	AudioTokens  uint64 `json:"audio_tokens"`  // Audio input tokens generated by the model.
	CachedTokens uint64 `json:"cached_tokens"` // Tokens generated by the model that were cached from previous completions.
}

var _ object.LLMUsage = (*Usage)(nil)

type Usage struct {
	TotalTokens             uint64                   `json:"total_tokens,omitempty"`              // Total number of tokens used in the request (prompt + completion).
	CompletionTokens        uint64                   `json:"completion_tokens,omitempty"`         // Number of tokens in the generated completion.
	PromptTokens            uint64                   `json:"prompt_tokens,omitempty"`             // Number of tokens in the prompt.
	CompletionTokensDetails *CompletionTokensDetails `json:"completion_tokens_details,omitempty"` // Breakdown of tokens used in a completion.
	PromptTokensDetails     *PromptTokensDetails     `json:"prompt_tokens_details,omitempty"`     // Breakdown of tokens used in the prompt.
}

func (u *Usage) GetTotalTokens() uint64 {
	return u.TotalTokens
}

func (u *Usage) GetCompletionTokens() uint64 {
	return u.CompletionTokens
}

func (u *Usage) GetPromptTokens() uint64 {
	return u.PromptTokens
}

var _ object.LLMResponse = (*ChatCompletionsResponse)(nil)

type ChatCompletionsResponse struct {
	Model  string         `json:"model"`
	Usage  *Usage         `json:"usage,omitempty"`
	Error  *ErrorResponse `json:"error,omitempty"`
	Stream bool           `json:"stream"`

	// TODO: add more fields

	request          object.LLMRequest
	responseBody     json.RawMessage
	bodyParsed       map[string]any
	outgoingResponse *http.Response
}

func NewChatCompletionResponse(request object.LLMRequest, response *http.Response, reader *bufio.Reader) (*ChatCompletionsResponse, error) {
	resp := new(ChatCompletionsResponse)

	buffer := new(bytes.Buffer)

	_, err := buffer.ReadFrom(reader)
	if err != nil {
		return nil, err
	}

	err = resp.processBytes(buffer.Bytes())
	if err != nil {
		return nil, fmt.Errorf("failed to unmarshal response: %w", err)
	}

	if resp.Error != nil {
		resp.Error.Status = response.StatusCode
	}

	resp.request = request
	resp.outgoingResponse = response

	return resp, nil
}

func (r *ChatCompletionsResponse) processBytes(bs []byte) error {
	r.responseBody = bs

	var body map[string]any

	err := json.Unmarshal(bs, &body)
	if err != nil {
		return fmt.Errorf("failed to unmarshal response body: %w", err)
	}

	r.bodyParsed = body

	r.Model = utils.GetByJSONPath[string](body, "{ .model }")
	usageMap := utils.GetByJSONPath[map[string]any](body, "{ .usage }")
	respErrMap := utils.GetByJSONPath[map[string]any](body, "{ .error }")

	usage, err := utils.FromMap[Usage](usageMap)
	if err != nil {
		return fmt.Errorf("failed to unmarshal usage: %w", err)
	}

	r.Usage = usage

	if len(respErrMap) > 0 {
		// todo
		// compatible with openai
		// server error type:
		//     "error": {
		//        "message": "Invalid credentials",
		//        "code": 401
		//    }
		// business error type:
		//  "error": {
		//        "type": "invalid_request_error",
		//        "code": "invalid_event",
		//        "message": "The 'type' field is missing.",
		//        "param": null,
		//        "event_id": "event_567"
		//    }
		respErr, err := utils.FromMap[Error](respErrMap)
		if err != nil {
			return fmt.Errorf("failed to unmarshal error: %w", err)
		}

		r.Error = &ErrorResponse{
			FromUpstream: true,
			ErrorBody:    respErr,
		}
	}

	return nil
}

func (r *ChatCompletionsResponse) MarshalJSON() ([]byte, error) {
	return json.Marshal(r.responseBody)
}

func (r *ChatCompletionsResponse) IsStream() bool {
	return false
}

func (r *ChatCompletionsResponse) GetRequestID() string {
	// TODO: implement
	return ""
}

func (r *ChatCompletionsResponse) GetModel() string {
	return r.Model
}

func (r *ChatCompletionsResponse) SetModel(model string) error {
	var err error

	r.responseBody, r.bodyParsed, err = modifyBytesBodyAndParsed(r.responseBody, NewReplace("/model", model))
	if err != nil {
		return err
	}

	r.Model = model

	return nil
}

func (r *ChatCompletionsResponse) GetUsage() object.LLMUsage {
	return r.Usage
}

func (r *ChatCompletionsResponse) GetOutgoingResponse() *http.Response {
	return r.outgoingResponse
}

func (r *ChatCompletionsResponse) GetError() error {
	if r.Error != nil {
		return r.Error
	}

	return nil
}
