package openai

import (
	"knoway.dev/pkg/object"
	"knoway.dev/pkg/utils"
)

type CompletionTokensDetails struct {
	AcceptedPredictionTokens uint64 `json:"accepted_prediction_tokens"` // When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion.
	AudioTokens              uint64 `json:"audio_tokens"`               // Audio input tokens generated by the model.
	ReasoningTokens          uint64 `json:"reasoning_tokens"`           // Tokens generated by the model for reasoning.
	RejectedPredictionTokens uint64 `json:"rejected_prediction_tokens"` // When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits.
}

type PromptTokensDetails struct {
	AudioTokens  uint64 `json:"audio_tokens"`  // Audio input tokens generated by the model.
	CachedTokens uint64 `json:"cached_tokens"` // Tokens generated by the model that were cached from previous completions.
}

var _ object.LLMUsage = (*ChatCompletionsUsage)(nil)
var _ object.LLMTokensUsage = (*ChatCompletionsUsage)(nil)

type ChatCompletionsUsage struct {
	object.IsLLMUsage

	TotalTokens             uint64                   `json:"total_tokens,omitempty"`              // Total number of tokens used in the request (prompt + completion).
	CompletionTokens        uint64                   `json:"completion_tokens,omitempty"`         // Number of tokens in the generated completion.
	PromptTokens            uint64                   `json:"prompt_tokens,omitempty"`             // Number of tokens in the prompt.
	CompletionTokensDetails *CompletionTokensDetails `json:"completion_tokens_details,omitempty"` // Breakdown of tokens used in a completion.
	PromptTokensDetails     *PromptTokensDetails     `json:"prompt_tokens_details,omitempty"`     // Breakdown of tokens used in the prompt.
}

func (u *ChatCompletionsUsage) GetTotalTokens() uint64 {
	return u.TotalTokens
}

func (u *ChatCompletionsUsage) GetCompletionTokens() uint64 {
	return u.CompletionTokens
}

func (u *ChatCompletionsUsage) GetPromptTokens() uint64 {
	return u.PromptTokens
}

func (u *ChatCompletionsUsage) GetOutputImages() []object.ImageGenerationsUsageImage {
	return make([]object.ImageGenerationsUsageImage, 0)
}

var _ object.ImageGenerationsUsageImage = (*ImageGenerationsUsageImage)(nil)

type ImageGenerationsUsageImage struct {
	Width   uint64 `json:"width,omitempty"`   // Width of the generated image.
	Height  uint64 `json:"height,omitempty"`  // Height of the generated image.
	Style   string `json:"style,omitempty"`   // Style of the generated image.
	Quality string `json:"quality,omitempty"` // Quality of the generated image.
}

func (i *ImageGenerationsUsageImage) GetWidth() uint64 {
	return i.Width
}

func (i *ImageGenerationsUsageImage) GetHeight() uint64 {
	return i.Height
}

func (i *ImageGenerationsUsageImage) GetStyle() string {
	return i.Style
}

func (i *ImageGenerationsUsageImage) GetQuality() string {
	return i.Quality
}

var _ object.LLMUsage = (*ImageGenerationsUsage)(nil)
var _ object.LLMImagesUsage = (*ImageGenerationsUsage)(nil)

type ImageGenerationsUsage struct {
	object.IsLLMUsage

	Images []*ImageGenerationsUsageImage `json:"images,omitempty"` // Usage information for each generated image.
}

func (u *ImageGenerationsUsage) GetTotalTokens() uint64 {
	return 0
}

func (u *ImageGenerationsUsage) GetCompletionTokens() uint64 {
	return 0
}

func (u *ImageGenerationsUsage) GetPromptTokens() uint64 {
	return 0
}

func (u *ImageGenerationsUsage) GetOutputImages() []object.ImageGenerationsUsageImage {
	return utils.TypeAssertFrom[*ImageGenerationsUsageImage, object.ImageGenerationsUsageImage](u.Images)
}
